# Data Engineering Project for Loading and Transforming Data from S3 to RDS

This project demonstrates a data pipeline that loads raw data from an S3 bucket, transforms it, and stores it in an RDS instance. The project is modularized into various Python scripts for connecting to RDS, creating tables, loading raw data, and transforming data.

## Project Structure

- **connectRDS.py**: Contains functions to establish and manage the connection to an RDS database using the SQLAlchemy library.
- **create_table.py**: Includes functions for creating tables in the RDS instance based on specified schema definitions.
- **loadraw.py**: Loads raw data from an external source (S3) into a staging table in the RDS instance.
- **transform.py**: Handles data transformation operations, including handling null values, performing groupby, pivot, melt, stack, DataFrame merges, concatenations, and unions.
- **main.py**: Orchestrates the entire ETL process by sequentially calling each module to connect, create tables, load raw data, and apply transformations.
- **S3.py**: Contains functions for interacting with S3, including fetching data and handling AWS credentials.
- **sp500_companies.csv**: Example dataset used in this project.
- **requirements.txt**: Lists the Python dependencies needed for the project. 

## Setup and Installation

### Prerequisites
- Python 3.x
- AWS credentials configured with S3 access permissions
- Access to an RDS database instance

### Install Dependencies
Use the following command to install the required dependencies:
```bash
pip install -r requirements.txt
```

### Environment Variables
Create a `.env` file in the root directory to store sensitive information like AWS credentials and database connection details.

Example:
```plaintext
AWS_ACCESS_KEY_ID=your_access_key
AWS_SECRET_ACCESS_KEY=your_secret_key
RDS_HOST=your_rds_host
RDS_USER=your_rds_username
RDS_PASS=your_rds_password
RDS_DB=your_database_name
```

## Usage

1. **Connecting to RDS**: `connectRDS.py` will establish a connection to the RDS database.
2. **Creating Tables**: Run `create_table.py` to create necessary tables in the RDS instance.
3. **Loading Data**: Use `loadraw.py` to load raw data from S3 into the RDS staging table.
4. **Transforming Data**: `transform.py` will apply various transformations to the data.
5. **Running the ETL Process**: Execute `main.py` to run the entire pipeline from connection to transformation.

## Dependencies

The dependencies are listed in `requirements.txt`:
```plaintext
pymysql
pandas
sqlalchemy
logging
boto3
python-io
os-sys
dotenv
```

## Contributing

Feel free to open issues or pull requests for improvements and fixes.

## License

This project is licensed under the MIT License.
